{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24f7e84",
   "metadata": {},
   "source": [
    "# Motion Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc305a19",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e79f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "import sys\n",
    "import xlsxwriter\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Importing Data preprocessing functions\n",
    "sys.path.append('data_processing/')\n",
    "from readDataset import dataGrabber\n",
    "from preProcessing import preProcess\n",
    "from dataPreparation import dataPrepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f87923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cuda\n"
     ]
    }
   ],
   "source": [
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"The device is: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3b081",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Either based on location id or recording id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff62ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Edit to some other number to load a different dataset\\nlocations_sel = ['4']\\n\\n# Initialize data Grabber Object\\ndata_obj = dataGrabber(dataset_path)\\n\\ndata_obj.location_id = locations_sel\\ndata_obj.read_csv_with_location()\\n\\ntrack_data_raw = data_obj.get_tracks_data()\\ntrack_meta_data_raw = data_obj.get_tracksMeta_data()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset based on recording location\n",
    "#Then uncomment and run this cell. Skip next cell\n",
    "\n",
    "dataset_path = 'dataset/data/'\n",
    "\n",
    "\"\"\"\n",
    "# Edit to some other number to load a different dataset\n",
    "locations_sel = ['4']\n",
    "\n",
    "# Initialize data Grabber Object\n",
    "data_obj = dataGrabber(dataset_path)\n",
    "\n",
    "data_obj.location_id = locations_sel\n",
    "data_obj.read_csv_with_location()\n",
    "\n",
    "track_data_raw = data_obj.get_tracks_data()\n",
    "track_meta_data_raw = data_obj.get_tracksMeta_data()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295d24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset based on recording id\n",
    "#Then uncomment and run this cell. Skip previous cell\n",
    "\n",
    "dataset_path = 'dataset/data/'\n",
    "\n",
    "recording_id_sel = ['27']\n",
    "\n",
    "# Initialize data Grabber Object\n",
    "data_obj = dataGrabber(dataset_path)\n",
    "\n",
    "data_obj.recording_id = recording_id_sel\n",
    "data_obj.read_csv_with_recordingID()\n",
    "\n",
    "track_data_raw = data_obj.get_tracks_data()\n",
    "track_meta_data_raw = data_obj.get_tracksMeta_data()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f1425",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b9f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Encoded\n",
      "0 : bicycle\n",
      "1 : car\n",
      "2 : pedestrian\n",
      "3 : truck_bus\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data preprocessing using the class preProcess()\n",
    "'''\n",
    "pre_process_obj = preProcess()\n",
    "pre_process_obj.tracks_data = track_data_raw\n",
    "pre_process_obj.tracks_meta_data = track_meta_data_raw\n",
    "pre_process_obj.recording_ids = data_obj.recording_id\n",
    "pre_process_obj.data_len = len(track_data_raw)\n",
    "\n",
    "'''\n",
    "Downsampling Data\n",
    "\n",
    "# Define the number of frames to be skipped + 1 => here 4 frames are skipped so 4+1 = 5\n",
    "'''\n",
    "pre_process_obj.frames_skipped = 5\n",
    "track_data_downsampled, tracks_meta_data = pre_process_obj.get_down_sampled_data()\n",
    "pre_process_obj.tracks_data = track_data_downsampled\n",
    "\n",
    "'''\n",
    "Encode objects (0: bicycle, 1 : car, 2 : pedestrian, 3 : truck_bus)\n",
    "'''\n",
    "pre_process_obj.label_encoding()\n",
    "pre_process_obj.print_label_encoder_classes()\n",
    "\n",
    "#Normalize data\n",
    "tracks_data_norm, min_max_scalar_list = pre_process_obj.normalize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9aacd",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4523d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while!\n",
      "Current progress: 0.0 %\n",
      "Current progress: 1.01 %\n",
      "Current progress: 2.02 %\n",
      "Current progress: 3.03 %\n",
      "Current progress: 4.04 %\n",
      "Current progress: 5.05 %\n",
      "Current progress: 6.06 %\n",
      "Current progress: 7.07 %\n",
      "Current progress: 8.08 %\n",
      "Current progress: 9.09 %\n",
      "Current progress: 10.1 %\n",
      "Current progress: 11.11 %\n",
      "Current progress: 12.12 %\n",
      "Current progress: 13.13 %\n",
      "Current progress: 14.14 %\n",
      "Current progress: 15.15 %\n",
      "Current progress: 16.16 %\n",
      "Current progress: 17.17 %\n",
      "Current progress: 18.18 %\n",
      "Current progress: 19.19 %\n",
      "Current progress: 20.2 %\n",
      "Current progress: 21.21 %\n",
      "Current progress: 22.22 %\n",
      "Current progress: 23.23 %\n",
      "Current progress: 24.24 %\n",
      "Current progress: 25.25 %\n",
      "Current progress: 26.26 %\n",
      "Current progress: 27.27 %\n",
      "Current progress: 28.28 %\n",
      "Current progress: 29.29 %\n",
      "Current progress: 30.3 %\n",
      "Current progress: 31.31 %\n",
      "Current progress: 32.32 %\n",
      "Current progress: 33.33 %\n",
      "Current progress: 34.34 %\n",
      "Current progress: 35.35 %\n",
      "Current progress: 36.36 %\n",
      "Current progress: 37.37 %\n",
      "Current progress: 38.38 %\n",
      "Current progress: 39.39 %\n",
      "Current progress: 40.4 %\n",
      "Current progress: 41.41 %\n",
      "Current progress: 42.42 %\n",
      "Current progress: 43.43 %\n",
      "Current progress: 44.44 %\n",
      "Current progress: 45.45 %\n",
      "Current progress: 46.46 %\n",
      "Current progress: 47.47 %\n",
      "Current progress: 48.48 %\n",
      "Current progress: 49.49 %\n",
      "Current progress: 50.51 %\n",
      "Current progress: 51.52 %\n",
      "Current progress: 52.53 %\n",
      "Current progress: 53.54 %\n",
      "Current progress: 54.55 %\n",
      "Current progress: 55.56 %\n",
      "Current progress: 56.57 %\n",
      "Current progress: 57.58 %\n",
      "Current progress: 58.59 %\n",
      "Current progress: 59.6 %\n",
      "Current progress: 60.61 %\n",
      "Current progress: 61.62 %\n",
      "Current progress: 62.63 %\n",
      "Current progress: 63.64 %\n",
      "Current progress: 64.65 %\n",
      "Current progress: 65.66 %\n",
      "Current progress: 66.67 %\n",
      "Current progress: 67.68 %\n",
      "Current progress: 68.69 %\n",
      "Current progress: 69.7 %\n",
      "Current progress: 70.71 %\n",
      "Current progress: 71.72 %\n",
      "Current progress: 72.73 %\n",
      "Current progress: 73.74 %\n",
      "Current progress: 74.75 %\n",
      "Current progress: 75.76 %\n",
      "Current progress: 76.77 %\n",
      "Current progress: 77.78 %\n",
      "Current progress: 78.79 %\n",
      "Current progress: 79.8 %\n",
      "Current progress: 80.81 %\n",
      "Current progress: 81.82 %\n",
      "Current progress: 82.83 %\n",
      "Current progress: 83.84 %\n",
      "Current progress: 84.85 %\n",
      "Current progress: 85.86 %\n",
      "Current progress: 86.87 %\n",
      "Current progress: 87.88 %\n",
      "Current progress: 88.89 %\n",
      "Current progress: 89.9 %\n",
      "Current progress: 90.91 %\n",
      "Current progress: 91.92 %\n",
      "Current progress: 92.93 %\n",
      "Current progress: 93.94 %\n",
      "Current progress: 94.95 %\n",
      "Current progress: 95.96 %\n",
      "Current progress: 96.97 %\n",
      "Current progress: 97.98 %\n",
      "Current progress: 98.99 %\n",
      "Current progress: 100.0 %\n",
      "Done! \n"
     ]
    }
   ],
   "source": [
    "# Resetting dropped frames index\n",
    "tracks_data_norm = tracks_data_norm.reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "Data preperation using the class dataPrepare()\n",
    "'''\n",
    "data_prepare_obj = dataPrepare()\n",
    "data_prepare_obj.tracks_data_norm = tracks_data_norm\n",
    "data_prepare_obj.tracksMeta_data = tracks_meta_data\n",
    "data_prepare_obj.data_len = len(tracks_data_norm)\n",
    "\n",
    "'''\n",
    "Choose the number of track id to be used and split the data\n",
    "'''\n",
    "# Number for track id to be used\n",
    "data_prepare_obj.track_id_range = 100\n",
    "\n",
    "data_prepare_obj.data_input = \"normalized_data\"\n",
    "xTrain_data, xTest_data, yTrain_data, yTest_data = data_prepare_obj.get_test_train_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263af43",
   "metadata": {},
   "source": [
    "### Load model, data and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072e5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('models/')\n",
    "from models import FcnTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3492c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape data\n",
    "\n",
    "n_input = np.shape(xTrain_data)[1] * np.shape(xTrain_data)[2]\n",
    "xTrain = np.reshape(xTrain_data, (np.shape(xTrain_data)[0], n_input))\n",
    "n_output = np.shape(yTrain_data)[1] * np.shape(yTrain_data)[2]\n",
    "yTrain = np.reshape(yTrain_data, (np.shape(yTrain_data)[0], n_output))\n",
    "\n",
    "xTest = np.reshape(xTest_data, (np.shape(xTest_data)[0], n_input))\n",
    "yTest = np.reshape(yTest_data, (np.shape(yTest_data)[0], n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eabc6bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113467, 45)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537b3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tensors and prepare dataloader\n",
    "X_train = torch.FloatTensor(xTrain)\n",
    "X_test = torch.FloatTensor(xTest)\n",
    "y_train = torch.FloatTensor(yTrain)\n",
    "y_test = torch.FloatTensor(yTest)\n",
    "\n",
    "trainloader = DataLoader(X_train, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(X_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7a5eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FcnTorch(\n",
       "  (fc1): Linear(in_features=80, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (out): Linear(in_features=32, out_features=45, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check model\n",
    "torch.manual_seed(33)\n",
    "model = FcnTorch().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c603ded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  40960\n",
      "    512\n",
      " 131072\n",
      "    256\n",
      "  32768\n",
      "    128\n",
      "   8192\n",
      "     64\n",
      "   2048\n",
      "     32\n",
      "   1440\n",
      "     45\n",
      "_______\n",
      " 217517 <- Total trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e86e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function and optimizer\n",
    "\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7402a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1  loss:   0.580160\n",
      "epoch: 101  loss:   0.076768\n",
      "epoch: 201  loss:   0.027865\n",
      "epoch: 301  loss:   0.019568\n",
      "epoch: 401  loss:   0.017327\n",
      "epoch: 501  loss:   0.017452\n",
      "epoch: 601  loss:   0.016111\n",
      "epoch: 701  loss:   0.016537\n",
      "epoch: 801  loss:   0.016106\n",
      "epoch: 901  loss:   0.015660\n",
      "epoch: 1001  loss:   0.015019\n",
      "epoch: 1101  loss:   0.014866\n",
      "epoch: 1201  loss:   0.015211\n",
      "epoch: 1301  loss:   0.013901\n",
      "epoch: 1401  loss:   0.013854\n",
      "epoch: 1501  loss:   0.013170\n",
      "epoch: 1601  loss:   0.013085\n",
      "epoch: 1701  loss:   0.012336\n",
      "epoch: 1801  loss:   0.012290\n",
      "epoch: 1901  loss:   0.012219\n",
      "epoch: 2001  loss:   0.011819\n",
      "epoch: 2101  loss:   0.012010\n",
      "epoch: 2201  loss:   0.012022\n",
      "epoch: 2301  loss:   0.011039\n",
      "epoch: 2401  loss:   0.012372\n",
      "epoch: 2501  loss:   0.011334\n",
      "epoch: 2601  loss:   0.010912\n",
      "epoch: 2701  loss:   0.011374\n",
      "epoch: 2801  loss:   0.011602\n",
      "epoch: 2901  loss:   0.010664\n",
      "epoch: 3001  loss:   0.010412\n",
      "epoch: 3101  loss:   0.010921\n",
      "epoch: 3201  loss:   0.011053\n",
      "epoch: 3301  loss:   0.010717\n",
      "epoch: 3401  loss:   0.011076\n",
      "epoch: 3501  loss:   0.010791\n",
      "epoch: 3601  loss:   0.010998\n",
      "epoch: 3701  loss:   0.010088\n",
      "epoch: 3801  loss:   0.010437\n",
      "epoch: 3901  loss:   0.010156\n",
      "epoch: 4001  loss:   0.010228\n",
      "epoch: 4101  loss:   0.009980\n",
      "epoch: 4201  loss:   0.010040\n",
      "epoch: 4301  loss:   0.009905\n",
      "epoch: 4401  loss:   0.010807\n",
      "epoch: 4501  loss:   0.009651\n",
      "epoch: 4601  loss:   0.010257\n",
      "epoch: 4701  loss:   0.009697\n",
      "epoch: 4801  loss:   0.009696\n",
      "epoch: 4901  loss:   0.009691\n",
      "epoch: 4999  loss:   0.009466\n",
      "\n",
      "Duration: 609 seconds\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5000\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "#     i+=1\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    loss = torch.sqrt(criterion(y_pred, y_train))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    #print losses for every 100 epochs\n",
    "    if i%100 == 1:\n",
    "        print(f'epoch: {i:3}  loss: {loss.item():10.6f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3}  loss: {loss.item():10.6f}') \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0252e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train.cpu(), y_train.cpu()\n",
    "y_pred = y_pred.cpu().detach().numpy()\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8f47b",
   "metadata": {},
   "source": [
    "### Generating Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "689aff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_book_filename = 'NN_velocity_prediction_result.xlsx' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c8bedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on recording id\n",
    "\n",
    "recording_id_sel = ['28']\n",
    "data_sel_id = 0 # If there are multiple recordings added\n",
    "\n",
    "# Initialize data Grabber Object\n",
    "test_data_obj = dataGrabber(dataset_path)\n",
    "\n",
    "test_data_obj.recording_id = recording_id_sel\n",
    "test_data_obj.read_csv_with_recordingID()\n",
    "\n",
    "test_track_data = test_data_obj.get_tracks_data()\n",
    "test_track_meta_data = test_data_obj.get_tracksMeta_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6973f9ee",
   "metadata": {},
   "source": [
    "Preprocessing and downsampling data to match training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c0117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre_process_obj = preProcess()\n",
    "test_pre_process_obj.tracks_data = test_track_data\n",
    "test_pre_process_obj.tracks_meta_data = test_track_meta_data\n",
    "test_pre_process_obj.recording_ids = test_data_obj.recording_id\n",
    "test_pre_process_obj.data_len = len(test_track_data)\n",
    "\n",
    "test_pre_process_obj.frames_skipped = 5\n",
    "test_track_data_downsampled, test_tracks_meta_data = test_pre_process_obj.get_down_sampled_data()\n",
    "test_pre_process_obj.tracks_data = test_track_data_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c6768",
   "metadata": {},
   "source": [
    "Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e226ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the tracks data normalized\n",
    "test_tracks_data_norm, min_max_scalar_list = test_pre_process_obj.normalize_data()\n",
    "# Resetting dropped frames index\n",
    "test_tracks_data_norm = test_tracks_data_norm.reset_index(drop=True)\n",
    "\n",
    "# Saving Normalized Data\n",
    "test_data_prepare_obj = dataPrepare()\n",
    "test_data_prepare_obj.tracks_data_norm = test_tracks_data_norm\n",
    "test_data_prepare_obj.tracksMeta_data = test_tracks_meta_data\n",
    "test_data_prepare_obj.data_len = len(test_tracks_data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604ccf0",
   "metadata": {},
   "source": [
    "Data stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9092fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while!\n",
      "Current progress: 0.0 %\n",
      "Current progress: 11.11 %\n",
      "Current progress: 22.22 %\n",
      "Current progress: 33.33 %\n",
      "Current progress: 44.44 %\n",
      "Current progress: 55.56 %\n",
      "Current progress: 66.67 %\n",
      "Current progress: 77.78 %\n",
      "Current progress: 88.89 %\n",
      "Current progress: 100.0 %\n",
      "Done! \n"
     ]
    }
   ],
   "source": [
    "# Number for track id to be used\n",
    "test_data_prepare_obj.track_id_range = 10  \n",
    "\n",
    "# Gets the tracks data normalized and its ID\n",
    "test_data_prepare_obj.data_input = \"normalized_data\"\n",
    "t_norm_Ids, t_in_norm, t_out_norm = test_data_prepare_obj.data_stacking()\n",
    "# Predict the output\n",
    "n_input = np.shape(t_in_norm)[1] * np.shape(t_in_norm)[2]\n",
    "t_in_norm_reshaped = np.reshape(t_in_norm, (np.shape(t_in_norm)[0], n_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121b79f",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4b54eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('evaluation/')\n",
    "from pred_evaluator import Evaluation\n",
    "from evaluation_matrix import evaluationMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f3dd84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while!\n",
      "Current progress: 0.0 %\n",
      "Current progress: 11.11 %\n",
      "Current progress: 22.22 %\n",
      "Current progress: 33.33 %\n",
      "Current progress: 44.44 %\n",
      "Current progress: 55.56 %\n",
      "Current progress: 66.67 %\n",
      "Current progress: 77.78 %\n",
      "Current progress: 88.89 %\n",
      "Current progress: 100.0 %\n",
      "Done! \n"
     ]
    }
   ],
   "source": [
    "data_eval_obj = Evaluation()\n",
    "\n",
    "# Resetting dropped frames index\n",
    "test_track_data_downsampled = test_track_data_downsampled.reset_index(drop=True)\n",
    "ground_truth_prepare_obj = dataPrepare()\n",
    "ground_truth_prepare_obj.data_input = \"raw_data\"\n",
    "ground_truth_prepare_obj.track_id_range = 10\n",
    "ground_truth_prepare_obj.tracksMeta_data = test_tracks_meta_data\n",
    "ground_truth_prepare_obj.tracks_data_norm = test_tracks_data_norm\n",
    "ground_truth_prepare_obj.tracks_data = test_track_data_downsampled\n",
    "ground_truth_prepare_obj.data_len = len(test_track_data_downsampled) \n",
    "#ground_truth_prepare_obj.num_predict = 15\n",
    "t_raw_Ids, t_in_raw, t_out_raw = ground_truth_prepare_obj.data_stacking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c086e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_obj.t_raw_Ids = t_raw_Ids\n",
    "data_eval_obj.t_in_raw = t_in_raw\n",
    "data_eval_obj.t_out_raw = t_out_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d14e0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xCenter_gt, yCenter_gt, heading_gt = data_eval_obj.get_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "811269d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(torch.FloatTensor(t_in_norm_reshaped))\n",
    "    \n",
    "# Save Predicted Data into the Evaluator\n",
    "data_eval_obj.y_hat = yhat.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "631010b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Paramters\n",
    "data_eval_obj.min_max_scalar_list = min_max_scalar_list\n",
    "\n",
    "# Get Prediction\n",
    "xCenter_prediction, yCenter_prediction, heading_prediction = data_eval_obj.get_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9eef2",
   "metadata": {},
   "source": [
    "Storing the generated predictions in a Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d5cdc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the file if it already exists\n",
    "if os.path.exists(work_book_filename):\n",
    "    os.remove(work_book_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ed0aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_obj.wb_filename = work_book_filename\n",
    "data_eval_obj.write_to_workbook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd56dd1",
   "metadata": {},
   "source": [
    "ade -- Average displacement error\n",
    "\n",
    "fde -- Final displacement error\n",
    "\n",
    "ahe -- Average absolute heading error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2019f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average displacement error is 2.267 m\n",
      "The average final displacement error is 2.021 m\n",
      "The average absolute heading error is 4.72 degrees\n"
     ]
    }
   ],
   "source": [
    "eval_obj = evaluationMatrix(work_book_filename, data_eval_obj.n_predict)\n",
    "ade_value, fde_value, ahe_val = eval_obj.get_fde_ade_ahe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6cac1",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aae99a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'PathPredModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c0c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project] *",
   "language": "python",
   "name": "conda-env-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
